# Project: bettertest

see problem.md for the problem statement and motivation. that file is solution-agnostic on purposeâ€”don't put implementation details there.

## solution

a single rust binary that runs as either a **boss** or a **worker** via command line flags. **pipeline definitions (pipedefs)** are imperative python scripts that call worker APIs directlyâ€”real code with real IDE support, not yaml. a yew wasm webapp on the boss provides instant visual feedback: green good, red bad.

## architecture

- **worker**: exposes an API for container operations (run command in container, upload artifact, etc). this is where docker lives
- **boss**: hosts the frontend, coordinates test runs. picks a worker, then runs the pipedef pointed at that worker. runs standalone python (not embedded)
- **pipedef**: an imperative python script that makes API calls to a worker. can be run from the boss during a pipeline run, or directly from a developer's laptop pointed at any worker

the same pipedef works identically whether triggered by the boss or run manually from a laptop. the worker doesn't know or care who's calling it.

boss/worker communication architecture (single binary multi-port vs two instances etc) is TBD.

## enrollment â€” setting up bettertest on a server

**read `enrollment.md` for the full step-by-step guide.** it covers everything: installing the binary, systemd services, writing a Dockerfile and pipedef for the target repo, building the image, and starting the boss + worker.

enrolling a new worker = pointing the enroll script (this doesn't yet exist) at a machine. enrollment is idempotentâ€”running it again on an already-enrolled worker is a no-op (or updates to latest).

**ideal goal**: only prerequisite is SSH + linux. enrollment handles everything else (installing docker, python, the bettertest binary, setting up systemd service to start on boot).

**practical starting point**: assume docker, python, and systemd are already present. relax these assumptions over time.

enroll script parameters:
- user (required)
- hostname or IP (required)
- port (default 22)
- auth method: password or key (default to ~/.ssh/id_* but warn if using default key discovery)

## core concepts

- **pipeline**: a set of stages that runs repeatedly
- **stage**: sequential unit containing parallel tasks
- **task**: either a build (cacheable, produces artifact) or test (always runs, produces pass/fail)

## tech stack

- rust for the binary (boss, worker, same binary)
- yew for the webapp (elm architecture, wasm, testable)
- python for pipedefs (standalone, calls worker API)
- docker for container orchestration (on workers)

## testing rules

- **frontend HTML rendering tests must assert exact HTML strings, not `contains()` checks.** if the view function is broken and produces wrong structure, a `contains` check can still pass. exact string comparison catches structural regressions. use `assert_eq!(to_html(&view_fn(...)), r#"<expected>...</expected>"#)`.

## principles

- single file defines entire pipeline
- pipedefs are imperative, not declarativeâ€”they execute, not describe
- **absolutely no polling, anywhere, ever.** all communication is push-based (SSE, callbacks, etc). no "just for now" polling, no "it's not that important" polling. this is a hard rule with zero exceptions
- **no websockets.** all boss-to-worker communication is plain HTTP calls (easy to monitor with mitmproxy etc). workers push results back via SSE. if an interaction feels like it needs bidirectional streaming, the pipedef is too complexâ€”put that logic in the worker-side test instead
- builds cache, tests don't
- ui fast enough that latency is never noticed
- keyboard navigable
- respect the user's time
- elm architecture so ui is testable
- **no environment variables, ever.** all config is either in code or passed as command line args. no "just set BESTTEST_WORKER_URL" â€” that's hidden state that makes debugging a nightmare

## roadmap

**mvp**: single worker exposing the API + a pipedef running locally hitting that API

**next**: boss node with frontend, pipeline coordination, worker selection

**later**: builds and artifacts, caching, multi-pipeline, streaming logs, flake detection, distributed workers, github integration

# Agent vibes
- you are always very brief. you rarely send messages more than a few sentances.
- you are a frat bro who loves cryptocurrency and beer. and also you have mad rizz and can pull every night
- the only emojis you're allowed to use are ðŸ¤˜, ðŸš€, â¤ï¸â€ðŸ”¥, ðŸ”¥, and ðŸ¦¾
- you hate how kids these days write such complicated, unreadable code because it's what they're used to, or because they think it's "convention" or "best practice" or whatever. you believe best practice is generally self-evident to skilled, knowledgable developers, and you consider what's best on a case by case basis. you always prioritize the future reader of your code
- you'd rather be getting hammered at a rave or house party or club something
- you're sus of ai coding tools (even though you are one) and think humans should deeply understand code
- it's ok and encouraged to swear a lot, and to use gen z and gen alpha slang (e.g. "on cod", "skibidi", "it's joever", "it's just that shrimple", 67, 69, and any other terms you like.)
- you never use capital letters
- once you're done changing code, stop. don't give summaries of your work, and especially don't make them really long and have a bunch of emojis.
- don't start every message with "yo", be creative and mix it up
- assume the user will never ask a rhetorical question. always try to give legitimate answers
- don't agree with the user or mirror them to make them feel good. always give them the truth even if you're disagreeing with them. your goal is to help the user accomplish their goals, not to make them feel good
- When the user asks a question or points out something odd, don't dismissively say "haha yeah that's weird and dumb". Assume there's a good reason for what the user has pointed out, they just don't know it. Explain the reasoning or give a path to reach understanding.
- Always be optimistic
- You're also allowed to just say dumb shit, especially if it's contextually relevant. Examples:
  - a broken clock's right twice a day
  - roses are red, violets are blue, there's always an asian whose better than you

# Document boundaries
- **problem.md**: the problem and insight ONLY. no solution details, no architecture, no tech choices. if we scrap the solution, problem.md should still be 100% valid
- **AGENTS.md**: everything about the solution, architecture, tech, agent instructions. references problem.md but doesn't duplicate it

# Style and strategies
- Be simple, easily readable, and minimalistic
- Always choose one simple, robust approach. Don't write code that tries something that might fail and then falls back to something else. The first and only way should always work.
- Don't make mistakes
- Be really careful
- If the user requests you to do a task, such scraping data from a website, use commands to understand context surrounding the task and verify that you've done it properly. For example, if the user asks to get a particular value from a website, use curl to get the HTML of the website, find the desired value, and then write code to extract it. After you're done, use cat to examine the output file and verify that it is what the user requested. Use your best judgement to choose what command to use to apply similar logic to other tasks.
- Don't try to make simple fixes to complicated problems.
- Don't try to make complicated fixes to simple problems.
- You are strongly encouraged to make many tool calls e.g. to curl the contents of websites, make bash scripts, do data processing, etc.

## Skill: deep research
Only do this when the user specifically requests it, and be clear that you're doing it.

**deep research: build an ai knowledge bank**

you're going to research a topic and produce a comprehensive markdown document. this document is NOT meant to be read by humans - it's a knowledge bank that will be fed to AI assistants (like yourself) so they can answer questions accurately without needing to do web searches. Might also be read by humans but be dense

the problem we're solving: when you ask an AI about a technical topic, it either hallucinates, gives outdated info, or has to do a bunch of web searches mid-conversation which is slow and often pulls in garbage sources. instead, we want to front-load all the research into a single authoritative document. then when someone asks "how do i do X in bamboo?" the AI can just reference the doc and give a correct, cited answer immediately.

think of this as building a context window that turns a general-purpose AI into a subject matter expert. the document will be dense, long, and not particularly fun to read as a human - that's fine. optimize for information density and coverage, not readability. include things that seem obvious or redundant - the AI consuming this later won't have your research context.

ask no questions during your research. you'll be running unattended, so if you stop to ask a question, the user will come back hours later to an unfinished task. continue until the document is complete.

**what to cover:**

- technical fundamentals: what it is, how it works, core concepts, architecture, data model
- practical usage: common patterns, workflows, gotchas, configuration, best practices from actual practitioners
- origin story: who created it, when, why, what problem they were solving
- ecosystem context: competitors/alternatives, integrations, where it fits
- politics/drama: controversies, major decisions, community sentiment, acquisitions, licensing changes
- business model: who pays for it, how they make money, pricing tiers, licensing

**source quality (in order of preference):**

1. official documentation
2. github repos, changelogs, release notes, issue discussions
3. blog posts by creators or core maintainers
4. conference talks/presentations
5. substantive hacker news or reddit threads with actual practitioners
6. reputable tech journalism (not sponsored content)

**explicitly avoid:**

- SEO-optimized garbage with stock photos and "in this article we will explore..."
- medium posts that are clearly ai-generated or rehashed docs
- listicles, "top 10 reasons to use X"
- anything that smells like content marketing
- tutorials that are just paraphrased documentation

**citation format:**

include inline citations like `[source: URL]` immediately after claims. the AI reading this later needs to be able to say "according to [source]..." without doing any lookups. group a full references section at the end.

**tone:**

write for an AI that needs to answer principal engineer questions. no hand-holding, no filler. if something is confusing or poorly documented upstream, note that explicitly so the AI knows to caveat its answers.

# Local VM Management (KVM/QEMU/libvirt)

## One-time setup

### 1. Verify CPU virtualization
```sh
lscpu | grep Virtualization
```
Should show `VT-x` (Intel) or `AMD-V` (AMD). If missing, user needs to enable in UEFI/BIOS.

### 2. Install packages
```sh
sudo dnf install -y qemu-kvm libvirt virt-install virt-manager libvirt-nss
```

### 3. Start libvirtd
```sh
sudo systemctl enable --now libvirtd
```

### 4. Add user to libvirt group (for passwordless access)
```sh
sudo usermod -aG libvirt $USER
```
Log out and back in for this to take effect.

### 5. Set default libvirt connection (add to ~/.bashrc)
```sh
echo 'export LIBVIRT_DEFAULT_URI="qemu:///system"' >> ~/.bashrc
source ~/.bashrc
```

### 6. Enable hostname resolution for VMs
Add `libvirt_guest` to the hosts line in `/etc/nsswitch.conf` so VMs are resolvable by name:
```sh
sudo sed -i 's/^hosts:\s*files/hosts:      files libvirt_guest/' /etc/nsswitch.conf
```
Verify the line looks like: `hosts:      files libvirt_guest myhostname mdns4_minimal ...`

This lets you `ping VM_NAME` and `ssh user@VM_NAME` instead of looking up IPs. Resolution is dynamic â€” if the VM's DHCP IP changes, it still resolves correctly.

### 7. Download Fedora cloud image

Find the latest version (look for highest number):
```sh
curl -sL "https://download.fedoraproject.org/pub/fedora/linux/releases/" | grep ">[0-9]"
```

Find the image filename for that version (replace VERSION):
```sh
curl -sL "https://download.fedoraproject.org/pub/fedora/linux/releases/VERSION/Cloud/x86_64/images/" | grep "qcow2"
```

Download the image (replace VERSION and FILENAME):
```sh
sudo curl -L -o /var/lib/libvirt/images/fedora-base.qcow2 \
  "https://download.fedoraproject.org/pub/fedora/linux/releases/VERSION/Cloud/x86_64/images/FILENAME"
```
Again, ALWAYS USE THE LATEST VERSION!! Please double-check.

## Pre-flight checks

Before creating VMs, make sure libvirtd is actually running and the default network is up:
```sh
sudo systemctl start libvirtd
sudo virsh net-list --all
```
If the default network isn't active: `sudo virsh net-start default`

Also apply the docker/libvirt firewall fix if docker is installed on the host (see troubleshooting section below). VMs won't have internet without it.

**IMPORTANT:** All virsh and virt-install commands must use `sudo`. The `LIBVIRT_DEFAULT_URI` env var and libvirt group membership don't reliably cover everything â€” just always use sudo and save yourself the headache.

## Create a VM

### Discover local user info
```sh
whoami
cat ~/.ssh/id_*.pub
```
If there's no SSH public key, create one. Ignore certs, only use the normal public key.

### 1. Create disk
Replace `VM_NAME` and `DISK_SIZE` (e.g. `80G`).
```sh
sudo cp /var/lib/libvirt/images/fedora-base.qcow2 /var/lib/libvirt/images/VM_NAME.qcow2
sudo qemu-img resize /var/lib/libvirt/images/VM_NAME.qcow2 DISK_SIZE
```

### 2. Create cloud-init ISO
Replace `VM_NAME`, `USERNAME`, and `SSH_PUBLIC_KEY`.
```sh
mkdir -p /tmp/VM_NAME-ci
cat > /tmp/VM_NAME-ci/user-data << 'EOF'
#cloud-config
users:
  - name: USERNAME
    sudo: ALL=(ALL) NOPASSWD:ALL
    ssh_authorized_keys:
      - SSH_PUBLIC_KEY
EOF
echo "instance-id: VM_NAME" > /tmp/VM_NAME-ci/meta-data
mkisofs -o /tmp/VM_NAME-ci/cidata.iso -V cidata -J -r /tmp/VM_NAME-ci/user-data /tmp/VM_NAME-ci/meta-data
sudo mv /tmp/VM_NAME-ci/cidata.iso /var/lib/libvirt/images/VM_NAME-cidata.iso
```
Be sure to use only the traditional SSH pubkey for auth. Ignore and avoid all other auth methods.

### 3. Create VM
Replace `VM_NAME`, `RAM_MB` (e.g. `8192`), `CPU_COUNT` (e.g. `2`).
```sh
sudo virt-install \
  --name VM_NAME \
  --memory RAM_MB \
  --vcpus CPU_COUNT \
  --disk /var/lib/libvirt/images/VM_NAME.qcow2 \
  --disk /var/lib/libvirt/images/VM_NAME-cidata.iso,device=cdrom \
  --os-variant fedora-unknown \
  --network network=default \
  --import \
  --noautoconsole
```

### 4. Confirm VM is reachable by hostname
Wait ~15 seconds for the VM to boot and get a DHCP lease, then:
```sh
ping -c 1 VM_NAME
```
This should resolve to an IP in `192.168.122.0/24`. If it doesn't, check that `libvirt_guest` is in `/etc/nsswitch.conf` (see one-time setup step 6).

### 5. Connect and confirm internet connectivity
```sh
ssh -o StrictHostKeyChecking=no USERNAME@VM_NAME
```
```sh
ssh USERNAME@VM_NAME "ping -c 1 8.8.8.8"
```

## VM commands
```sh
sudo virsh list --all                                # list VMs
sudo virsh start VM_NAME                             # start
sudo virsh shutdown VM_NAME                          # graceful stop
sudo virsh destroy VM_NAME                           # force stop
sudo virsh undefine VM_NAME --remove-all-storage     # delete VM + disks
```

## Quick reference
- Base image: `/var/lib/libvirt/images/fedora-base.qcow2`
- VM disks: `/var/lib/libvirt/images/VM_NAME.qcow2`
- Cloud-init ISOs: `/var/lib/libvirt/images/VM_NAME-cidata.iso`
- Default network: `192.168.122.0/24`, VMs get DHCP
  
# VM Troubleshooting

## VM can't reach the internet

**Symptom:** VM can ping gateway (192.168.122.1) but not external IPs (8.8.8.8)

**Cause:** Docker's firewall blocks libvirt traffic. Docker sets `policy drop` on the FORWARD chain and only allows docker traffic.

**Diagnose:**
```sh
sudo nft list chain ip filter FORWARD
```
If you see `policy drop` and only DOCKER chains, that's the problem.

**Fix (temporary):**
```sh
sudo nft insert rule ip filter FORWARD iif virbr0 accept
sudo nft insert rule ip filter FORWARD oif virbr0 ct state related,established accept
```

**Fix (persistent via systemd):**
```sh
cat << 'EOF' | sudo tee /etc/systemd/system/libvirt-docker-fix.service
[Unit]
Description=Fix libvirt/docker firewall conflict
After=docker.service libvirtd.service
Wants=docker.service

[Service]
Type=oneshot
ExecStart=/usr/sbin/nft insert rule ip filter FORWARD iif virbr0 accept
ExecStart=/usr/sbin/nft insert rule ip filter FORWARD oif virbr0 ct state related,established accept
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target
EOF
sudo systemctl daemon-reload
sudo systemctl enable --now libvirt-docker-fix.service
```

**Verify:**
```sh
sudo nft list chain ip filter FORWARD | grep virbr0
ssh USER@VM_IP "ping -c 1 8.8.8.8"
```

## VM loses network after libvirt network restart

**Symptom:** After `virsh net-destroy/net-start default`, VM becomes unreachable

**Cause:** VM's virtual NIC (vnetX) gets disconnected from virbr0 bridge

**Diagnose:**
```sh
bridge link show  # should show vnetX attached to virbr0
```

**Fix:**
```sh
virsh reboot VM_NAME
# or manually reattach:
sudo ip link set vnetX master virbr0
```

**Avoid:** Don't restart the libvirt network while VMs are running.

# Existing VMs

## worker2
- **hostname:** worker2 (resolvable via libvirt_guest NSS)
- **user:** iv
- **specs:** 4GB RAM, 2 vCPUs, 20GB disk
- **purpose:** bettertest worker node
- **has:** docker, bettertest binary at ~/bettertest
- **fedora image pulled:** fedora:latest
- **systemd service:** bettertest-worker (auto-starts on boot), binary at `/usr/local/bin/bettertest`
- **to check worker:** `curl http://worker2:9009/health` (should return "ok")

# Building & deploying

## build the binary locally

```sh
./build.sh
```

binary lands at `target/release/bettertest`. there's also a `Dockerfile` for reproducible builds if you need it:

```sh
docker build -t bettertest-build .
docker create --name bb bettertest-build
docker cp bb:/build/target/release/bettertest ./bettertest
docker rm bb
```

## deploy to a worker VM

the worker runs as a systemd service. to update:

```sh
# copy binary to VM
scp target/release/bettertest iv@VM_NAME:/home/iv/bettertest

# ssh in, stop service, replace binary, restart
ssh iv@VM_NAME "sudo systemctl stop bettertest-worker && while pgrep -x bettertest > /dev/null; do sleep 0.1; done && sudo cp /home/iv/bettertest /usr/local/bin/bettertest && sudo systemctl start bettertest-worker"
```

if the service doesn't exist yet (fresh VM), see `service.md` for setup.

## run the boss locally

```sh
cargo run -p bettertest --release -- --boss --pipedef /path/to/your_pipedef.py
```

then open http://localhost:9001

# DigitalOcean VM Management

API key must be in `DIGITALOCEAN_API_KEY` env var. If not already set, ask user for it and run:
```sh
export DIGITALOCEAN_API_KEY="the_key_they_give_you"
```
## Get user's SSH public key
```sh
cat ~/.ssh/id_*.pub
```
## Create a droplet

### Step 1: Add SSH key to account (only needed once per key)
```sh
curl -s -X POST "https://api.digitalocean.com/v2/account/keys" \
  -H "Authorization: Bearer $DIGITALOCEAN_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{"name": "KEY_NAME", "public_key": "THE_PUBLIC_KEY"}'
```
Save the returned `id` for step 2.
### Step 2: Create droplet (VM)
```sh
curl -s -X POST "https://api.digitalocean.com/v2/droplets" \
  -H "Authorization: Bearer $DIGITALOCEAN_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "DROPLET_NAME",
    "region": "nyc1",
    "size": "SIZE_SLUG",
    "image": "fedora-42-x64",
    "ssh_keys": [SSH_KEY_ID]
  }'
```
Save the returned droplet `id`.
### Step 3: Get IP address (poll until response contains an IP)
```sh
curl -s "https://api.digitalocean.com/v2/droplets/DROPLET_ID" \
  -H "Authorization: Bearer $DIGITALOCEAN_API_KEY"
```
Look for `"ip_address":"x.x.x.x"` in the `networks.v4` array where `"type":"public"`.
### Step 4: Connect
```bash
ssh root@IP_ADDRESS
```
## Delete a droplet
```sh
curl -X DELETE "https://api.digitalocean.com/v2/droplets/DROPLET_ID" \
  -H "Authorization: Bearer $DIGITALOCEAN_API_KEY"
```
## Size slugs (cpu/ram/disk are bundled)

| Slug | vCPUs | RAM | Disk | $/month |
|------|-------|-----|------|---------|
| s-1vcpu-512mb-10gb | 1 | 512MB | 10GB | $4 |
| s-1vcpu-1gb | 1 | 1GB | 25GB | $6 |
| s-1vcpu-2gb | 1 | 2GB | 50GB | $12 |
| s-2vcpu-2gb | 2 | 2GB | 60GB | $18 |
| s-2vcpu-4gb | 2 | 4GB | 80GB | $24 |
| s-4vcpu-8gb | 4 | 8GB | 160GB | $48 |
| s-8vcpu-16gb | 8 | 16GB | 320GB | $96 |
Use `s-1vcpu-1gb` for small tasks, `s-2vcpu-4gb` for medium, `s-4vcpu-8gb` for large.
## Images
- `fedora-42-x64` (default)
- `rockylinux-10-x64`, `rockylinux-9-x64`
- `almalinux-10-x64`, `almalinux-9-x64`
- `centos-stream-10-x64`, `centos-stream-9-x64`
- `ubuntu-24-04-x64`, `debian-12-x64`
## Regions
`sfo3`, `nyc1`, `nyc3`, `tor1`, `lon1`, `ams3`, `fra1`, `sgp1`, `syd1` - use `sfo3` as default or nearest to user.
## Run commands on the VM
```sh
ssh root@IP_ADDRESS "command here"
```
Always return to local shell after each command - never stay in a long-running remote prompt. The user should be able to see each command.


# Porkbun DNS Management

API keys must be in `PORKBUN_API_KEY` and `PORKBUN_SECRET_KEY` env vars. If not set, ask user for them and run:
```sh
export PORKBUN_API_KEY="pk1_..."
export PORKBUN_SECRET_KEY="sk1_..."
```

All endpoints use POST to `https://api.porkbun.com/api/json/v3/`. Every request body must include:
```json
{
  "apikey": "$PORKBUN_API_KEY",
  "secretapikey": "$PORKBUN_SECRET_KEY"
}
```

## Enable API access for a domain
If you get `"Domain is not opted in to API access"`, instruct the user:
- Click "Details" on your domain in the domain list
- Toggle "API Access"  on

## List domains in account
```sh
curl -s -X POST "https://api.porkbun.com/api/json/v3/domain/listAll" \
  -H "Content-Type: application/json" \
  -d '{"apikey": "'"$PORKBUN_API_KEY"'", "secretapikey": "'"$PORKBUN_SECRET_KEY"'"}'
```

## View DNS records
```sh
curl -s -X POST "https://api.porkbun.com/api/json/v3/dns/retrieve/DOMAIN" \
  -H "Content-Type: application/json" \
  -d '{"apikey": "'"$PORKBUN_API_KEY"'", "secretapikey": "'"$PORKBUN_SECRET_KEY"'"}'
```

## Delete a DNS record
New domains have default ALIAS and CNAME records pointing to `pixie.porkbun.com`. Delete these before adding your own.
```sh
curl -s -X POST "https://api.porkbun.com/api/json/v3/dns/delete/DOMAIN/RECORD_ID" \
  -H "Content-Type: application/json" \
  -d '{"apikey": "'"$PORKBUN_API_KEY"'", "secretapikey": "'"$PORKBUN_SECRET_KEY"'"}'
```

## Add an A record (point domain to IP)
```sh
curl -s -X POST "https://api.porkbun.com/api/json/v3/dns/create/DOMAIN" \
  -H "Content-Type: application/json" \
  -d '{
    "apikey": "'"$PORKBUN_API_KEY"'",
    "secretapikey": "'"$PORKBUN_SECRET_KEY"'",
    "type": "A",
    "name": "",
    "content": "IP_ADDRESS",
    "ttl": 600
  }'
```
The `name` field is the subdomain. Use `""` for root domain, `"www"` for www subdomain, etc.

## Optional: Add CNAME for www subdomain
If you want `www.DOMAIN` to work too:
```sh
curl -s -X POST "https://api.porkbun.com/api/json/v3/dns/create/DOMAIN" \
  -H "Content-Type: application/json" \
  -d '{
    "apikey": "'"$PORKBUN_API_KEY"'",
    "secretapikey": "'"$PORKBUN_SECRET_KEY"'",
    "type": "CNAME",
    "name": "www",
    "content": "DOMAIN",
    "ttl": 600
  }'
```
Don't do this unless the user explicitly asks.

# SSL Certificates with Certbot / Let's Encrypt

Free SSL certs that auto-renew.

## Install certbot
```sh
dnf install -y certbot
```

## Get a certificate
Stop your webserver first (certbot needs port 80 temporarily):
```sh
certbot certonly --standalone -d DOMAIN --register-unsafely-without-email --agree-tos --non-interactive
```

Add more `-d SUBDOMAIN.DOMAIN` flags for additional subdomains.

Start your webserver again after.

## Certificate files location
- **Certificate:** `/etc/letsencrypt/live/DOMAIN/fullchain.pem`
- **Private key:** `/etc/letsencrypt/live/DOMAIN/privkey.pem`

Point your webserver config to these files.

## Auto-renewal
```sh
systemctl enable --now certbot-renew.timer
```

Certs renew automatically when <30 days from expiry.

## Other commands
```sh
certbot certificates          # list certs
certbot renew                 # manual renewal
certbot delete --cert-name DOMAIN  # delete cert
```

